---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Functions needed to run:
```{r fullmodel-fit-indices}
MLX2<-function(submodels,saturated.submodels,data){
  #submodels is a list containing the submodels in your model
  # in the form of linear, generalized linear, generalized additive
  # mixed models, or any other model object that has AIC and
  #log-likelihood attributes.
  #saturated.submodels is a list containing the submodels of your
  # model that defines the saturated submodels (or otherwise)
  #into which your model is properly nested.
  #data is the data set
  #
  #number of submodels in full model
  nobs<-dim(data)[1]
  nsub<-length(submodels)
  error.mes1<-error.mes2<-rep(FALSE,nsub)
  #if there is an error in estimating a model, then error.mes==TRUE
  #and don't calculate statistics
  for(i in 1:nsub){
    #check if the submodels and the saturated submodels are
    #of the same class, and stop if not.
    stopifnot(class(submodels[[i]])==
                class(saturated.submodels[[i]]))
    if(all(class(submodels[[i]])=="lm")){
      error.mes1[i]<-error.mes2[i]<-FALSE
    }
    else{
      error.mes1[i]<-!submodels[[i]]$converged & submodels[[i]]$boundary
      error.mes2[i]<-!saturated.submodels[[i]]$converged & saturated.submodels[[i]]$boundary
    }}
  out<-data.frame(submodel=1:nsub,logLikelihoods=rep(NA,nsub),
                  k=rep(NA,nsub),AICs=rep(NA,nsub),n.free.parameters=
                    rep(NA,nsub))
  out.saturated<-data.frame(submodel=1:nsub,logLikelihoods=rep(NA,nsub),
                            k=rep(NA,nsub),AICs=rep(NA,nsub),
                            n.free.parameters=rep(NA,nsub))
  
  #get likelihoods, AIC & k and store in "out"
  for(i in 1:nsub){
    out$logLikelihoods[i]<-logLik(submodels[[i]])
    out$AICs[i]<-AIC(submodels[[i]])
    out$k[i]<-nobs-df.residual(submodels[[i]])
    out$n.free.parameters[i]<-attributes(logLik(submodels[[i]]))$df
    out.saturated$n.free.parameters[i]<-attributes(logLik(saturated.submodels[[i]]))$df
        out.saturated$logLikelihoods[i]<-logLik(saturated.submodels[[i]])
    out.saturated$AICs[i]<-AIC(saturated.submodels[[i]])
    out.saturated$k[i]<-nobs-df.residual(saturated.submodels[[i]])
  }
  #Overall k, AIC and likelihood for models:
  model.AIC<-sum(out$AIC)
  model.LL<-sum(out$logLikelihoods)
#model df is the difference in the # of free parameters in the
#less constrained model relative to the # of free parameters in the
# more constrained (nested) model
  model.df<-sum(out.saturated$n.free.parameters)-
    sum(out$n.free.parameters)
  n.free.parameters<-sum(out$n.free.parameters)
  n.saturated.free.parameters<-sum(out.saturated$n.free.parameters)
  saturated.model.AIC<-sum(out.saturated$AIC)
  saturated.model.LL<-sum(out.saturated$logLikelihoods)
  saturated.model.df<-sum(out.saturated$k)
# the MLX2 statistic is the difference in likelihoods between the
#more constrained (nested) model and the less constrained model;
#usually the saturated model
  X2<--2*(model.LL-saturated.model.LL)
  if(X2<0)X2<-NA
#  df<-saturated.model.df-model.df
  null.prob<-NA
#Only calculate null prob if the X2 is valid with valid df
  if(!is.na(X2)&(model.df>0))null.prob<-1-pchisq(X2,model.df)
  #check if any models had errors in estimation
  error.flag<-sum(error.mes1)+sum(error.mes2)
  list(model.X2=X2,model.df=model.df,null.prob=null.prob,
       model.loglikelihood=model.LL,
       n.free.parameters=n.free.parameters,
       n.free.parameters.saturated=n.saturated.free.parameters,
       saturated.model.loglikelihood=saturated.model.LL,
       model.AIC=model.AIC,submodels=submodels,error.flag=error.flag)
}
```
#Function for calculating effective sample size from Moran's I and actual sample size
```{r effective-sample-size}
N.eff<-function(I,N){ #input values: Moran's I estimate, variable sample size
  result<-N*(1-I)/(1+I) #calculate n' using Cressie 2015's equation
  return(result)
}
```
#Function for calculating adjusted model parameters
```{r model-adjuster}
mod.adjuster<-function(mod,N.eff){ #input values: model object to be adjusted, n' value
  std.errors<-coef(summary(mod))[,2]
  estimates<-coef(summary(mod))[,1] #store standard errors and coefficient estimates of model
  
  N<-length(mod$fitted.values) #pull out variable sample size from model
  adj.index<-sqrt(N/N.eff)
  
  SE.adj<-std.errors*adj.index #adjust standard errors
  t.adj<-estimates/SE.adj
  p.adj<-2*pt(-abs(t.adj),df = N.eff - 1) #adjust p values using adjusted t values
  
  df.results<-data.frame(SE.adj,p.adj)
  return(df.results)
}
```
#Function for revising the dataframe to reflect spot-checked photos
```{r df-reviser}
df.reviser<-function(file,df,colnum = NA){  #function for revising datasets to reflect spot checked values
  
bot.rev<-read.delim(file = file)
int.mod<-df

print(paste("Number of false positives found:",sum(bot.rev[,3] == 0)))

if(is.na(colnum) == FALSE){
  print(paste("Number of positives in original dataframe:",sum(as.numeric(df[,colnum] > 0))))
}

for (i in 1:length(bot.rev[,1])) {
  response<-bot.rev[i,3]
  
  if (response != 0 & response != 1){
    print("mistakes were made")
    print(response)
    print(bot.rev[i,])
  }

  if (response == 0){
    row<-bot.rev[i,1]
    DSC<-bot.rev[i,2]
    
    
    for (j in 1:length(df[,1])) {
      ref.row<-df$Row
        ref.row<-ref.row[j]
      ref.DSC<-df$Image_Id
        ref.DSC<-ref.DSC[j]
      if (row == ref.row & DSC == ref.DSC){
        int.mod[j,colnum]<-response
      }
    }
  }
}
print(paste("Adjusted number of observations:", sum(as.numeric(int.mod[,colnum] > 0))))
  return(int.mod)
}
```
#Function for range adjusted coefficient calculation
```{r coef-stdizer}
range.std.coef<-function(PAmod,Pmod,data){ #takes presence/absence and abundance models for input, and the dataframe used for modelling
  PAcoef_names<-names(PAmod$coefficients)
    PAcoef_names<-PAcoef_names[-1]  #remove the intercept
  df_names<-names(data)
  
  PA_vars_stats<-matrix(ncol = length(PAcoef_names),nrow = 3)
  n<-0
for (h in 1:length(PAcoef_names)) {
  for (i in 1:length(df_names)) {
    if(df_names[i] == PAcoef_names[h]){
      n<-n + 1

      PA_vars_stats[1,n]<-mean(data[,i])  #stores the mean value of each predictor variable
      PA_vars_stats[2,n]<-min(data[,i])   #stores the min value
      PA_vars_stats[3,n]<-max(data[,i])   #stores the max value
      break
    }
  }
}
  
  Pcoef_names<-names(Pmod$coefficients)
    Pcoef_names<-Pcoef_names[-1]  #remove the intercept
  df_names<-names(data)
  
  P_vars_stats<-matrix(ncol = length(Pcoef_names),nrow = 3)
  
  m<-0
for (j in 1:length(Pcoef_names)) {
  for (k in 1:length(df_names)) {
    if(df_names[k] == Pcoef_names[j]){
      m<-m + 1
      
      P_vars_stats[1,m]<-mean(data[,k])  #stores the mean value of each predictor variable
      P_vars_stats[2,m]<-min(data[,k])   #stores the min value
      P_vars_stats[3,m]<-max(data[,k])   #stores the max value
      break
    }
  }
} 
  #find the range of the dependent variable in the models
  dep_var_name<-names(Pmod$model)[1] #use presence since PA is a binary dependent var
  for (b in 1:length(df_names)) {
    if(df_names[b] == dep_var_name){
      dep_var<-data[,b]
      
      var_range_vals<-range(data[,b])
      var_range<-var_range_vals[2] - var_range_vals[1]
      break
    }
  }  
  
#calculate max and min values for each predictor in the presence/absence model  
  PAcoef<-PAmod$coefficients
  PAmod_mean_vals<-c()
  PAmod_max_vals<-c()
  PAmod_min_vals<-c()
  
  for (z in 1:length(PAcoef_names)) {
    PAmod_mean_vals[z]<-PAcoef[z+1] * PA_vars_stats[1,z]
    PAmod_min_vals[z]<-PAcoef[z+1] * PA_vars_stats[2,z]
    PAmod_max_vals[z]<-PAcoef[z+1] * PA_vars_stats[3,z]
  }
#calculate max and min values for each predictor in the abundance model
  Pcoef<-Pmod$coefficients
  Pmod_mean_vals<-c()
  Pmod_max_vals<-c()
  Pmod_min_vals<-c()
  for (z in 1:length(Pcoef_names)) {
    Pmod_mean_vals[z]<-Pcoef[z+1] * P_vars_stats[1,z]
    Pmod_min_vals[z]<-Pcoef[z+1] * P_vars_stats[2,z]
    Pmod_max_vals[z]<-Pcoef[z+1] * P_vars_stats[3,z]
  }
  
##In the case of a model with differing sets of coefficients, different procedure is needed
 if ( length(Pcoef_names) != length(PAcoef_names)){ #test for difference
  
  if(length(PAcoef_names) > length(Pcoef_names)){#establish which model is longer
    long_list<-PAcoef_names
    id<-'PA'   #marker for later use
    alt.id<-'P'
    short_list<-Pcoef_names
  }
  else{
      long_list<-Pcoef_names
      short_list<-PAcoef_names
      id<-'P'
      alt.id<-'PA'
    }
  
  dupes<-match(x = long_list,short_list)  #find the positions of matching coefficients 
  dupeShort<-match(short_list,long_list)     
  
  
  #vectors to store calculated values
  max.toggle<-c()
  min.toggle<-c()
  diff.vec<-c()
  for (j in 1:length(dupes)) { #loop through the longer model
    if(is.na(dupes[j]) != TRUE){ #when this statement is TRUE, coef present in both models
      d<-dupes[j]
      if(id == 'PA'){ #a TRUE here indicates the presence/absence model is longer
        PAmax_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_max_vals[j] + sum(PAmod_mean_vals[-j]))))
        Pmax_toggle<-Pcoef[1] + Pmod_max_vals[d] + sum(Pmod_mean_vals[-d])
        
        PAmin_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_min_vals[j] + sum(PAmod_mean_vals[-j]))))
        Pmin_toggle<-Pcoef[1] + Pmod_min_vals[d] + sum(Pmod_mean_vals[-d])
        
        max.toggle[j]<-PAmax_toggle*Pmax_toggle
        min.toggle[j]<-PAmin_toggle*Pmin_toggle
        diff.vec[j]<-max.toggle[j] - min.toggle[j]
      }
      else{ #indicates that presence model is longer, so reverse the indexes
        Pmax_toggle<-Pcoef[1] + Pmod_max_vals[j] + sum(Pmod_mean_vals[-j])
        PAmax_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_max_vals[d] + sum(PAmod_mean_vals[-d]))))
        
        Pmin_toggle<-Pcoef[1] + Pmod_min_vals[j] + sum(Pmod_mean_vals[-j])
        PAmin_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_min_vals[d] + sum(PAmod_mean_vals[-d]))))
        
        max.toggle[j]<-PAmax_toggle*Pmax_toggle
        min.toggle[j]<-PAmin_toggle*Pmin_toggle
        diff.vec[j]<-max.toggle[j] - min.toggle[j]
      }
    }
  else{ #indicates that the coef is unique to the long model
    if(id == 'PA'){ #longer mod is PA
        PAmax_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_max_vals[j] + sum(PAmod_mean_vals[-j]))))
        Pmax_toggle<-Pcoef[1] + sum(Pmod_mean_vals) #no longer need to worry about max
        
        PAmin_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_min_vals[j] + sum(PAmod_mean_vals[-j]))))
        Pmin_toggle<-Pcoef[1] + sum(Pmod_mean_vals) # or mins
        
        max.toggle[j]<-PAmax_toggle*Pmax_toggle
        min.toggle[j]<-PAmin_toggle*Pmin_toggle
        diff.vec[j]<-max.toggle[j] - min.toggle[j]
      }
      else{ #the longer model is presence
        Pmax_toggle<-Pcoef[1] + Pmod_max_vals[j] + sum(Pmod_mean_vals[-j])
        PAmax_toggle<-1/(1+1/(exp(PAcoef[1] + sum(PAmod_mean_vals))))
        
        Pmin_toggle<-Pcoef[1] + Pmod_min_vals[j] + sum(Pmod_mean_vals[-j])
        PAmin_toggle<-1/(1+1/(exp(PAcoef[1] + sum(PAmod_mean_vals))))
        
        max.toggle[j]<-PAmax_toggle*Pmax_toggle
        min.toggle[j]<-PAmin_toggle*Pmin_toggle
        diff.vec[j]<-max.toggle[j] - min.toggle[j]
      }
  }  
  }
  #separate vectors for the short model coef
  max.toggleS<-c()
  min.toggleS<-c()
  diff.vecS<-c()
  for (j in 1:length(dupeShort)) { #this one loops through the shorter length mod
    if(is.na(dupeShort[j]) != TRUE){#TRUE here indicates coefficient is present in both mods
      d<-dupeShort[j]
      if(id != 'PA'){ #PA is shorter (switched the logic, kept indexing the same)
        PAmax_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_max_vals[j] + sum(PAmod_mean_vals[-j]))))
        Pmax_toggle<-Pcoef[1] + Pmod_max_vals[d] + sum(Pmod_mean_vals[-d])
        
        PAmin_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_min_vals[j] + sum(PAmod_mean_vals[-j]))))
        Pmin_toggle<-Pcoef[1] + Pmod_min_vals[d] + sum(Pmod_mean_vals[-d])
        
        max.toggleS[j]<-PAmax_toggle*Pmax_toggle
        min.toggleS[j]<-PAmin_toggle*Pmin_toggle
        diff.vecS[j]<-max.toggleS[j] - min.toggleS[j]
      }
      else{ #P is shorter
        Pmax_toggle<-Pcoef[1] + Pmod_max_vals[j] + sum(Pmod_mean_vals[-j])
        PAmax_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_max_vals[d] + sum(PAmod_mean_vals[-d]))))
        
        Pmin_toggle<-Pcoef[1] + Pmod_min_vals[j] + sum(Pmod_mean_vals[-j])
        PAmin_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_min_vals[d] + sum(PAmod_mean_vals[-d]))))
        
        max.toggleS[j]<-PAmax_toggle*Pmax_toggle
        min.toggleS[j]<-PAmin_toggle*Pmin_toggle
        diff.vecS[j]<-max.toggleS[j] - min.toggleS[j]
      }
    }
    else{ #coef is unique to the shorter model
    if(id != 'PA'){ #PA is shorter
        PAmax_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_max_vals[j] + sum(PAmod_mean_vals[-j]))))
        Pmax_toggle<-Pcoef[1] + sum(Pmod_mean_vals)
        
        PAmin_toggle<-1/(1+1/(exp(PAcoef[1] + PAmod_min_vals[j] + sum(PAmod_mean_vals[-j]))))
        Pmin_toggle<-Pcoef[1] + sum(Pmod_mean_vals)
        
        max.toggleS[j]<-PAmax_toggle*Pmax_toggle
        min.toggleS[j]<-PAmin_toggle*Pmin_toggle
        diff.vecS[j]<-max.toggleS[j] - min.toggleS[j]
      }
      else{ #P is shorter
        Pmax_toggle<-Pcoef[1] + Pmod_max_vals[j] + sum(Pmod_mean_vals[-j])
        PAmax_toggle<-1/(1+1/(exp(PAcoef[1] + sum(PAmod_mean_vals))))
        
        Pmin_toggle<-Pcoef[1] + Pmod_min_vals[j] + sum(Pmod_mean_vals[-j])
        PAmin_toggle<-1/(1+1/(exp(PAcoef[1] + sum(PAmod_mean_vals))))
        
        max.toggleS[j]<-PAmax_toggle*Pmax_toggle
        min.toggleS[j]<-PAmin_toggle*Pmin_toggle
        diff.vecS[j]<-max.toggleS[j] - min.toggleS[j]
        }
      }
  }  
  #divide by the variable's range to gain a vector of standardized coefficients
  std_long<-diff.vec/var_range
  std_short<-diff.vecS/var_range
  
  #put together cohesive object for return value
  long_name<-paste(id," model std coef")
  short_name<-paste(alt.id," model std coef")
  
  std_long<-data.frame(std_long,row.names = long_list)
    colnames(std_long)<-long_name
    
  std_short<-data.frame(std_short,row.names = short_list)
    colnames(std_short)<-short_name
  
  print(paste(long_list,"long mod:",max.toggle,min.toggle))
  print(paste(short_list,"short mod:",max.toggleS,min.toggleS))
  print(paste(long_list,"long mod:", std_long))
  print(paste(short_list, "short model:",std_short))
  
  result<-list(std_long,std_short)
    names(result)<-c(long_name,short_name)
  
  return(result)
     }

  #this code can carry out the simpler case of models with the same coefficients
  
  PAmax_toggle<-c()  #create vectors for maximum/minimum values of presence/absence model
  PAmin_toggle<-c()
  for (y in 1:length(PA_vars_stats[1,])) {
    PAmax_toggle[y]<-1/(1+1/(exp(PAcoef[1] + PAmod_max_vals[y] + sum(PAmod_mean_vals[-y]))))#need to transform from logit
    PAmin_toggle[y]<-1/(1+1/(exp(PAcoef[1] + PAmod_min_vals[y] + sum(PAmod_mean_vals[-y]))))
  }
  
  Pmax_toggle<-c()   #create vectors for maximum/minimum values of presence/absence model
  Pmin_toggle<-c()
  for (y in 1:length(P_vars_stats[1,])) {
    Pmax_toggle[y]<-Pcoef[1] + Pmod_max_vals[y] + sum(Pmod_mean_vals[-y])
    Pmin_toggle[y]<-Pcoef[1] + Pmod_min_vals[y] + sum(Pmod_mean_vals[-y])
  }
  #calculate the combined max/min toggles 
  max_toggle<-PAmax_toggle * Pmax_toggle
  min_toggle<-PAmin_toggle * Pmin_toggle
  
  #calculate the full model mean, and print it alongsidde dependent variable mean
  full_mod_mean<-(Pcoef[1] + sum(Pmod_mean_vals)) * 1/(1+1/(exp((PAcoef[1] + sum(PAmod_mean_vals)))))
  print(paste("Model mean:",signif(full_mod_mean,digits = 5))) #calculate and report the mean
  print(paste("Dependent Var. Mean: ",mean(dep_var)))
  
  diff_vector<-max_toggle - min_toggle  #take the difference between maximum and minimum toggle
  stdized_vector<-diff_vector/var_range #divide by variable range
    
  
  
    for (p in seq(length(stdized_vector))) { #print each of the standardized coefficients
      print(paste(PAcoef_names[p],"=",signif(stdized_vector[p],digits = 5),"(Range standardized)"))
    }
  
  result<-rbind(PAcoef_names,stdized_vector) #cohesive table to be returned
  
  return(result)
}
```

#BORRICHIA submodel
```{r data-read-in}
#file name is computer specific
preCNNdata<-read.csv("2014_plants_snail_xy_data.csv")
  preCNNdata<-na.omit(CNNdata)
```
#read in revised data and replace values
```{r}
CNNdata<-df.reviser('BatisCheck.txt',preCNNdata,12)  #use column 12 for Batis (this is all dataframe dependent)
CNNdata<-df.reviser('SpartCheck.txt',CNNdata,9)  #use column 9 for Spartina
CNNdata<-df.reviser('LimoCheck.txt',CNNdata,10)  #use column 10 for Limonium
CNNdata<-df.reviser('BorrCheck.txt',CNNdata,11)  #use column 11 for Borr
CNNdata<-df.reviser('JuncCheck.txt',CNNdata,13)  #use column 13 for Juncus
CNNdata<-df.reviser('SarcCheck.txt',CNNdata,8)  #use column 8 for Sarcocornia
```
#create a marker for the nonhospitable habitat (salt pan in the site interior)
```{r saltpan-marker}
#make a marker column for points in which no species are present and salinity is high
saltpan<-ifelse(CNNdata$Spart==0 & CNNdata$Sarc==0 & CNNdata$Limon==0 & CNNdata$Batis==0 & CNNdata$Bor==0 & CNNdata$Juncus==0 & CNNdata$salinity..psu.>60, 1,0)
#append these markers to our data frame
CNNdata$saltpanYN<-saltpan
```

##construct Borrichia submodel
```{r}
borr.thresh<-ifelse(CNNdata$Bor == 0 & CNNdata$elevation..m. < 0.67 & CNNdata$saltpanYN == 0, 1, 0)
#there are 3 instances of Bor presence below 0.66 m elevation (need to investigate this)

CNNdata$borrthreshYN<-borr.thresh  #here also, 0 indicates hospitable habitat
```
#insert PA/log abundance columns and subset
```{r}
CNNdata$borrPA<-ifelse(CNNdata$Bor > 0,1,0)
CNNdata$logBorr<-log1p(CNNdata$Bor)
#now subset the dataframe
borr.hab<-subset(CNNdata,borrthreshYN == 0 & saltpanYN == 0)
borr.inhosp<-rbind(subset(CNNdata,borrthreshYN == 1),subset(CNNdata, saltpanYN == 1))
#sum to 9278?
sum(length(borr.hab[,1]),length(borr.inhosp[,1]))
#sums over 9278, circle back and exclude saltpan obs in threshold object
```
#conduct logistic regression on the PA data with the hospitable habitat df
```{r}
borrPAmod1<-glm(borrPA~elevation..m. + salinity..psu. + Juncus,family = 'binomial',data = borr.hab)

library(MASS)
borrStep<-stepAIC(borrPAmod1,direction = "both")
borrStep$anova

#initial model accepted on revision
```

```{r}
#view selected model
summary(borrPAmod1)

#grab residuals for Moran's I testing
PA.resid<-resid(borrPAmod1)
```
#conduct loglinear regression on positive sarcocornia values
```{r}
borr.habY<-subset(borr.hab,borrPA==1)  #create a df for only obs where Borrichia is present

borrPmod1<-lm(logBorr~elevation..m. + salinity..psu. + Juncus,data = borr.habY)

borrStep2<-stepAIC(borrPmod1,direction = "both")
borrStep2$anova

#Salinity is removed by very small AIC difference (<1 point)
borrPmod2<-lm(logBorr~elevation..m. + Juncus,data = borr.habY)
```

```{r}
#view selected model
summary(borrPmod2)

#grab residuals for Moran's I testing
P.resid<-resid(borrPmod2)
```
#capture predictions for both models
```{r}
#create objects for loglinear prediction equation
elev<-borr.habY$elevation..m.
sal<-borr.habY$salinity..psu.
Junc<-borr.habY$Juncus


LLpred<- -0.20006 + 2.67711*elev - 0.06031*Junc 

borr.hab$borrLMpred<-0   
n<-0
for (i in 1:length(borr.hab[,1])) {   #index in the LM productivity estimates to sarc.hab
  test<-borr.hab$logBorr[i]
  if(test > 0 ){ #productivity estimate is zero where not present
    n<-n+1
    
    borr.hab$borrLMpred[i]<-LLpred[n]  
  }
}

#get logistic predictions from model
borr.hab$borrLOGpred<-predict(borrPAmod1,type = "response")

#insert 0 predictions for each value in the salt pan & below the elevation threshold
borr.inhosp$borrLOGpred<-0
borr.inhosp$borrLMpred<-0
```
#conduct spatial autocorrelation correction on both models
```{r}
library(spdep) #library the spatial dependance package

#carry out the Moran's I estimation for both models
PAxy.data<-cbind(borr.hab$easting..m.,borr.hab$northing..m.) 
Pxy.data<-cbind(borr.habY$easting..m.,borr.habY$northing..m.)

PAxy.knn<-knearneigh(PAxy.data,k = 4)
PAxy.nb<-knn2nb(PAxy.knn)

Pxy.knn<-knearneigh(Pxy.data,k = 4)
Pxy.nb<-knn2nb(Pxy.knn)

PA.borr.Moran<-moran.test(PA.resid,nb2listw(PAxy.nb,style ="W"))
P.borr.Moran<-moran.test(P.resid,nb2listw(Pxy.nb,style ="W"))
```

#compute adjusted sample size, standard errors and p values
```{r}
PAborrNeff<-as.numeric(N.eff(PA.borr.Moran$estimate[1],length(borr.hab[,1]))) #calculate n' for each model
PborrNeff<-as.numeric(N.eff(P.borr.Moran$estimate[1],length(borr.habY[,1])))


mod.adjuster(borrPAmod1,PAborrNeff) #adjust each model and print values
mod.adjuster(borrPmod2,PborrNeff)

#evaluate p values for increases above 0.05
```

#Continuing as directed in the Schwieger R code, may need to amend to reflect updates
```{r}
#bind the two dataframes together, and create a cohesive prediction value column
borr.merge<-rbind(borr.hab,borr.inhosp)

borr.merge$borrPRED<-borr.merge$borrLOGpred*borr.merge$borrLMpred
```
#regress our cohesive predictive value against the observed value
```{r}
borrFmod<-lm(borr.merge$logBorr ~ borr.merge$borrPRED)

summary(borrFmod)

#store R^2 value
borrR2<-summary(borrFmod)$r.squared
borrR2adj<-summary(borrFmod)$adj.r.squared
```
#calculate R2 and standardize coefficients
```{r}
borr_coef<-range.std.coef(borrPAmod1,borrPmod2,CNNdata)
```
#check the fuctions performance on models with different variables
```{r}
elev<-mean(CNNdata$elevation..m.)
sal<-mean(CNNdata$salinity..psu.)
Junc<-mean(CNNdata$Juncus)

maxElev<-max(CNNdata$elevation..m.)
maxSal<-max(CNNdata$salinity..psu.)
maxJunc<-max(CNNdata$Juncus)

minElev<-min(CNNdata$elevation..m.)
minSal<-min(CNNdata$salinity..psu.)
minJunc<-min(CNNdata$Juncus)

PAcoef<-borrPAmod1$coefficients
Pcoef<-borrPmod2$coefficients


maxElev<-(1/(1+1/(exp(PAcoef[1] + PAcoef[2]*maxElev + PAcoef[3]*sal + PAcoef[4]*Junc))))*(Pcoef[1] + Pcoef[2]*maxElev + Pcoef[3]*Junc)
print(paste("max Elev toggle:",maxElev))

maxSal<-(1/(1+1/(exp(PAcoef[1] + PAcoef[2]*elev + PAcoef[3]*maxSal + PAcoef[4]*Junc))))*(Pcoef[1] + Pcoef[2]*elev + Pcoef[3]*Junc)
print(paste("max Sal toggle:",maxSal))

maxJunc<-(1/(1+1/(exp(PAcoef[1] + PAcoef[2]*elev + PAcoef[3]*sal + PAcoef[4]*maxJunc))))*(Pcoef[1] + Pcoef[2]*elev + Pcoef[3]*maxJunc)
print(paste("max Junc toggle:",maxJunc))


minElev<-(1/(1+1/(exp(PAcoef[1] + PAcoef[2]*minElev + PAcoef[3]*sal + PAcoef[4]*Junc))))*(Pcoef[1] + Pcoef[2]*minElev + Pcoef[3]*Junc)
print(paste("min Elev toggle:",minElev))

minSal<-(1/(1+1/(exp(PAcoef[1] + PAcoef[2]*elev + PAcoef[3]*minSal + PAcoef[4]*Junc))))*(Pcoef[1] + Pcoef[2]*elev + Pcoef[3]*Junc)
print(paste("min Sal toggle:",minSal))

minJunc<-(1/(1+1/(exp(PAcoef[1] + PAcoef[2]*elev + PAcoef[3]*sal + PAcoef[4]*minJunc))))*(Pcoef[1] + Pcoef[2]*elev + Pcoef[3]*minJunc)
print(paste("min Junc toggle",minJunc))

diffElev<-maxElev - minElev
diffSal<-maxSal - minSal
diffJunc<-maxJunc - minJunc

rangeBorr<-range(CNNdata$logBorr)
  rangeBorr<-rangeBorr[2] - rangeBorr[1]

stdElev<-diffElev/rangeBorr
stdSal<-diffSal/rangeBorr
stdJunc<-diffJunc/rangeBorr

print(paste("Standardized Elevation:", stdElev))
print(paste("Standardized Salinity:", stdSal))
print(paste("Standardized Juncus:",stdJunc))
```
